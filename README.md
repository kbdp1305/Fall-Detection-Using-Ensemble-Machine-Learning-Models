# ğŸ§â€â™‚ï¸ TriDA: Trial-Channel Double Attention for Fall Detection Using Ensemble Machine-Learning Models  
### ğŸ† **2nd Place Winner â€” Data Slayer 2.0 National AI & Data Science Competition (Officially Held by Telkom University, 2024)**  
### ğŸ‘¥ Team : *Kalah Menang Tetap Nganggur*  
**Fauzan Ihza Fajar Â· Krisna Bayu Dharma Putra Â· Akmal Muzakki Bakir**

###  Run the Colab Notebook
> Attention-enhanced fall detection that fuses **pose**, **appearance**, and **motion** features via a **TriDA** block and an **ensemble** of lightweight classifiersâ€”built for accuracy *and* real-time inference.  

[â–¶ï¸ **Open the Full Colab Notebook**](https://colab.research.google.com/drive/1_v7Fdh5uu5uUF7F-kOrLDRYVDJoq_kwj?usp=sharing)

> **Note:** The complete notebook exceeds 25 MB and is hosted on Colab.  
---

## ğŸ“˜ Overview
**TriDA (Trial-Channel Double Attention)** is an **attention-enhanced fall-detection framework** that fuses **pose**, **appearance**, and **motion** information to distinguish true human falls from normal daily activities with high precision and real-time responsiveness.

This project was developed for the **Data Slayer 2.0 National Data Science and Machine Learning Competition**, **officially organized and hosted by Telkom University, Indonesia**.  
Our team, *Kalah Menang Tetap Nganggur*, earned **ğŸ† 2nd place among 220 teams nationwide**, recognized for achieving exceptional accuracy and inference speed across unseen datasets.

---

## ğŸ¯ Motivation
Falls remain one of the primary causes of severe injury and mortality among the elderly. Traditional vision-based systems struggle with:

- âŒ False positives caused by similar daily actions (e.g., sitting or bending)  
- âŒ Sensitivity to lighting and camera angles  
- âŒ Latency on real-time embedded devices  

**TriDA** addresses these challenges using:

- âœ³ï¸ **Dual-attention modules (CBAM)** to refine spatial and channel features  
- ğŸ©» **Multi-stream pose estimation (Mediapipe BlazePose + YOLOv8-Pose)**  
- ğŸ§­ **Spine orientation vectors** for motion and directional context  
- âš™ï¸ **Ensemble tree models (Random Forest, XGBoost, LightGBM, CatBoost)** for robust final classification  

---

## ğŸ§© Methodology

### âš™ï¸ Workflow Pipeline
```text
Input Video  
   â†“  
Person Detection (ResNet + CBAM)  
   â†“  
Pose Estimation (Mediapipe BlazePose + YOLOv8-Pose)  
   â†“  
Spine Vector Calculation  
   â†“  
Feature Fusion via TriDA Block  
   â†“  
Ensemble Classifier â†’ Fall / Non-Fall
```

---

### ğŸ§  1ï¸âƒ£ Pose and Feature Extraction
| Component | Framework | Function |
|:--|:--|:--|
| **YOLOv8-Pose** | Ultralytics | Detects person and extracts 17 keypoints from frames in real time |
| **Mediapipe BlazePose GHUM Heavy** | Google Mediapipe | Produces 33-landmark skeleton with fine-grained spatial precision |
| **Spine Vector Module** | Custom | Computes spine angle and ratio to analyze postural orientation |
| **CBAM (Convolutional Block Attention Module)** | Integrated with ResNet-50 | Adds spatial and channel attention for better feature selection |

---

### ğŸ§© 2ï¸âƒ£ Feature Integration
Each frame is processed into three feature groups â€“ pose, visual, and geometric â€“ then merged using the **Trial-Channel Double Attention (TriDA)** block:  

\[
F_{TriDA} = CBAM(F_{CNN}) \oplus Pose(Mediapipe, YOLOv8) \oplus SpineVector
\]

The output is a unified representation fed to an ensemble of classifiers to predict fall vs non-fall events robustly under class imbalance conditions.

---

### ğŸ§  3ï¸âƒ£ Data Augmentation
To compensate for minority class scarcity (fall samples â‰ˆ 15 %), the team applied **ADASYN synthetic oversampling**.  
This balanced dataset improved recall and reduced bias toward non-fall activities.

| Distribution | Before | After ADASYN |
|:--|:--:|:--:|
| Fall samples | ~1 200 | ~4 000 |  
| Non-Fall samples | ~8 000 | ~8 000 |  

---

## ğŸ§® Modeling and Architecture

| Model | Feature Source | F1 Score | Notes |
|:--|:--|:--:|:--|
| YOLOv8-Pose Only | Pose Keypoints | 0.9232 | Baseline vision model |
| Mediapipe-Pose Only | Pose Landmarks | 0.9350 | Higher skeleton coverage |
| Mediapipe + YOLOv8 | Dual Pose Fusion | 0.9554 | Complementary features |
| **Proposed TriDA** | Multi-source Fusion + Ensemble | **0.9962** | Best model â€” 99.8 % acc / F1 |

**Ensemble models:** Random Forest, CatBoost, XGBoost, and LightGBM with majority voting achieved final F1 = 0.998 and accuracy = 0.998 on validation data.

---

## ğŸ” Evaluation Highlights
- **Cross-validation:** 5-fold split (75 % train Â· 15 % val Â· 10 % test)  
- **Metric:** F1-score (primary), Accuracy and Precision as supporting  
- **Public Leaderboard:** 0.9969 F1 | **Private Leaderboard:** 0.9938 F1  
- **Average Inference Latency:** < 0.08 s per frame (T4 GPU)

---

## ğŸš€ Results and Performance
- ğŸ§  **TriDA outperformed all baseline models by 4â€“6 % F1 margin**.  
- âš¡ **Real-time inference (> 12 FPS)** achieved without hardware optimization.  
- ğŸ©º **Spine vector + pose fusion** significantly reduced false positives in â€œsittingâ€ and â€œbendingâ€ actions.  
- ğŸ§¾ **Attention visualization maps** showed that CBAM focused on critical body regions (head and torso) during fall frames.

---

## ğŸŒ Impact
- Provides a scalable solution for **elderly care, health monitoring, and smart CCTV** applications.  
- Architecture is lightweight enough for **edge devices (Raspberry Pi + Jetson Nano)**.  
- Enables real-time alerts and analytics dashboards for nursing homes and IoT systems.

---

## âš™ï¸ Technical Stack
```python
Python 3.10  
OpenCV, NumPy, Pandas, Matplotlib  
PyTorch, TensorFlow, Ultralytics YOLOv8  
Mediapipe, LightGBM, XGBoost, CatBoost, Scikit-learn  
ADASYN (SMOTE variant), CBAM attention module
```

---

## ğŸ’» Usage
### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/<your-username>/TriDA-FallDetection.git
cd TriDA-FallDetection
```

### 2ï¸âƒ£ Run the Colab Notebook
> Attention-enhanced fall detection that fuses **pose**, **appearance**, and **motion** features via a **TriDA** block and an **ensemble** of lightweight classifiersâ€”built for accuracy *and* real-time inference.  

[â–¶ï¸ **Open the Full Colab Notebook**](https://colab.research.google.com/drive/1_v7Fdh5uu5uUF7F-kOrLDRYVDJoq_kwj?usp=sharing)

> **Note:** The complete notebook exceeds 25 MB and is hosted on Colab.  

---

## ğŸ“œ Conclusion
- âœ… **TriDA successfully integrates CBAM, pose estimation, and ensemble learning** to achieve state-of-the-art fall detection accuracy (99.8 %).  
- ğŸš€ **2nd place winner at Data Slayer 2.0 Competition**, officially held by **Telkom University**.  
- ğŸ’¡ Demonstrates the potential of attention-based multi-stream fusion for real-time human-activity understanding.

---

## ğŸ‘¥ Authors
| Name | Role | Contribution |
|:--|:--|:--|
| **Krisna Bayu Dharma Putra** | Team Leader Â· Artificial Intelligence Engineer Â· Model Architect | TriDA block design, training pipeline, CBAM integration, Yolov8 pose extractor, multi-stream pose fusion|
| **Akmal Muzakki Bakir** | Artificial Intelligence Engineer | Mediapipe pose extractor and power point organizer|
| **Fauzan Ihza Fajar** | Researcher | Paper research |

ğŸ“§ [linkedin.com/in/dharma-putra1305](https://linkedin.com/in/dharma-putra1305)  
ğŸŒ [github.com/kbdp1305](https://github.com/kbdp1305)
